#!/usr/bin/python
__requires__ = 'TurboGears[future]'
import pkg_resources
pkg_resources.require("TurboGears")

import glob
import logging
import re
import optparse
import os
import stat
import string
import sys
import yum.repoMDObject
import datetime
import time
import hashlib

from sqlobject import *
import turbogears
from turbogears import config

sys.path.append('/usr/share/mirrormanager/server')
from mirrormanager.model import *
from mirrormanager.repomap import *
from mirrormanager.lib import manage_pidfile, remove_pidfile, run_rsync


rootdir='/'
pidfile='/var/run/mirrormanager/umdl.pid'
delete_directories=False
logger = None

from turbogears.database import PackageHub
hub = PackageHub("mirrormanager")
__connection__ = hub

def trim_os_from_dirname(dirname):
    if config.get('mirrormanager.projectname') == 'CentOS':
        return dirname
    # trim the /os off the name
    index = dirname.rfind('/os')
    if index > 0:
        dirname = dirname[:index]
    return dirname

def rename_SRPMS_source(l):
    rc = []
    for i in l:
        if i == 'source':
            pass
        elif i == 'SRPMS':
            rc.append('source')
        else:
            rc.append(i)
    return rc


def _get_version_from_path(path):
    # Debian/Ubuntu versioning
    s = r'dists/(\w+)/' # this ignores 10.10 and maverick-{anything}, but picks up 'maverick'
    m = re.search(re.compile(s), path)
    if m is not None:
        return m.group(1)
    # Fedora versioning
    s = r'/?(([\.\d]+)(\-\w+)?)/'
    m = re.search(re.compile(s), path)
    if m is not None:
        return m.group(1)
    return None

def create_version_from_path(category, path):
    ver = None
    vname = _get_version_from_path(path)
    if vname is not None and vname != '':
        if '/test/' in path:
            isTest = True
        else:
            isTest = False
        try:
            ver = Version(product=category.product, name=vname, isTest=isTest)
        except:
            pass

    return ver

arch_cache = dict()
version_cache = []

def setup_arch_version_cache():
    global arch_cache
    arch_cache = dict()
    for a in list(Arch.select()):
        arch_cache[a.name] = dict(name=a.name,
                                  id=a.id)

    global version_cache
    version_cache = []
    for v in list(Version.select(orderBy='-id')):
        version_cache.append(dict(id = v.id,
                                  name = v.name,
                                  product_id = v.product.id,
                                  ordered_mirrorlist = v.ordered_mirrorlist)
                             )

def guess_ver_arch_from_path(category, product_id, path):
    arch = None
    if 'SRPMS' in path:
        arch = arch_cache['source']
    else:
        for aname, v in arch_cache.iteritems():
            s = '.*(^|/)%s(/|$).*' % (aname)
            if re.compile(s).match(path):
                arch = aname
                break

    ver = None
    # newest versions/IDs first, also handles stupid Fedora 9.newkey hack.
    for v in version_cache:
        if v['product_id'] != product_id: continue
        s = '.*(^|/)%s(/|$).*' % (v['name'])
        if re.compile(s).match(path):
            ver = v
            break

    # create Versions if we can figure it out...
    if ver is None:
        v = create_version_from_path(category, path)
        if v:
            ver = dict(id = v.id, name = v.name,
                     product_id = product_id,
                     ordered_mirrorlist = v.ordered_mirrorlist)

            version_cache.append(ver)
    return (ver, arch)


# Something like this is committed to yum upstream, but may not be in the copy we are using.
def set_repomd_timestamp(yumrepo):
    timestamp = 0
    for ft in yumrepo.fileTypes():
        thisdata = yumrepo.repoData[ft]
        timestamp = max(int(thisdata.timestamp), timestamp)
    yumrepo.timestamp = timestamp
    return timestamp

def make_file_details_from_checksums(dname, D):
    def _parse_checksum_file(path, checksumlen):
        r = {}
        try:
            f = open(path, 'r')
            for line in f:
                line = line.strip()
                s = line.split()
                if len(s) < 2:
                    continue
                if len(s[0]) != checksumlen:
                    continue
                # strip off extraneous starting '*' char from name
                s[1] = s[1].strip('*')
                r[s[1]] = s[0]
            f.close()
        except:
            pass
        return r

    def _checksums_from_globs(dirname, globs, checksumlen):
        d = {}
        checksum_files = []
        for g in globs:
            checksum_files.extend(glob.glob(os.path.join(rootdir, dirname, g)))
        for f in checksum_files:
            d.update(_parse_checksum_file(f, checksumlen))
        return d

    sha1_globs = ['*.sha1sum', 'SHA1SUM']
    md5_globs = ['*.md5sum', 'MD5SUM']
    sha256_globs = ['*-CHECKSUM']
    sha512_globs = ['*.sha512sum', 'SHA512SUM']
    md5dict = _checksums_from_globs(dname, md5_globs, 32)
    sha1dict = _checksums_from_globs(dname, sha1_globs, 40)
    sha256dict = _checksums_from_globs(dname, sha256_globs, 64)
    sha512dict = _checksums_from_globs(dname, sha512_globs, 128)

    files = set()
    for k in md5dict.keys():
        files.add(k)
    for k in sha1dict.keys():
        files.add(k)
    for k in sha256dict.keys():
        files.add(k)
    for k in sha512dict.keys():
        files.add(k)

    for f in files:
        try:
            s = os.stat(os.path.join(rootdir, dname, f))
        except OSError:
            # bail if the file doesn't actually exist
            continue
        sha1 = sha1dict.get(f)
        md5  = md5dict.get(f)
        sha256  = sha256dict.get(f)
        sha512  = sha512dict.get(f)
        size = s.st_size
        ctime = s[stat.ST_CTIME]
        try:
            fd = FileDetail.selectBy(directory=D, filename=f, sha1=sha1, md5=md5, sha256=sha256, sha512=sha512,
                                     size=size, timestamp=ctime)[0]
        except IndexError:
            fd = FileDetail(directory=D, filename=f, sha1=sha1, md5=md5, sha256=sha256, sha512=sha512,
                            timestamp=ctime, size=size)
    

def make_repomd_file_details(dname, D):
    if not dname.endswith('/repodata'):
        return
    repomd_fname = os.path.join(rootdir, dname, 'repomd.xml')
    if not os.path.exists(repomd_fname):
        return
    try:
        f = open(repomd_fname, 'r')
        repomd = f.read()
        f.close()
    except:
        return
    size = len(repomd)
    md5 = hashlib.md5(repomd).hexdigest()
    sha1 = hashlib.sha1(repomd).hexdigest()
    sha256 = hashlib.sha256(repomd).hexdigest()
    sha512 = hashlib.sha512(repomd).hexdigest()

    yumrepo = yum.repoMDObject.RepoMD('repoid', repomd_fname)
    if 'timestamp' not in yumrepo.__dict__:
        set_repomd_timestamp(yumrepo)
    timestamp = yumrepo.timestamp
    try:
        fd = FileDetail.selectBy(directory=D, filename='repomd.xml', sha1=sha1, md5=md5, sha256=sha256, sha512=sha512,
                                 timestamp=timestamp, size=size)[0]
    except IndexError:
        fd = FileDetail(directory=D, filename='repomd.xml', sha1=sha1, md5=md5, sha256=sha256, sha512=sha512,
                        timestamp=timestamp, size=size)


def move_repository_from_development(prefix, arch, newdir):
    try:
        repo = Repository.selectBy(prefix=prefix, arch=arch)[0]
    except:
        return None

    if repo.directory is not None and newdir.name is not None:
        if '/development' in repo.directory.name and '/releases' in newdir.name:
            repo.directory = newdir
    return repo

def make_repository(dir, dname, category, product_id, topdir):
    repo = None
    path = dname[len(topdir)+1:]
    (ver, arch) = guess_ver_arch_from_path(category, product_id, path)
    if ver is None or arch is None:
        return None

    # stop making duplicate Repository objects.
    if len(dir.repositories) > 0:
        return None

    prefix = repo_prefix(path, category, ver)
    try:
	# historically, Repository.name was a longer string with
	# product and category deliniations.  But we were getting
	# unique constraint conflicts once we started introducing
	# repositories under repositories.  And .name isn't used for
	# anything meaningful.  So simply have it match dir.name,
	# which can't conflict.
        repo = Repository(name=dir.name, category=category, version=ver, arch=arch, directory=dir, prefix=prefix)
        logger.info('Created Repository %s -> Directory %s' % (repo, dir))
    except:
        # repo exists, but it may be one we want to move
        #try:
        #repo = move_repository_from_development(prefix, arch, dir)
        #except:
        #pass
        pass

    return repo

def nuke_gone_directories(rootdir):
    """ deleting a Directory has a ripple effect through the whole
        database.  Be really sure you're ready do to this.  It comes
        in handy when say a Test release is dropped."""
        
    for d in Directory.select(orderBy='-name'):
        dname = d.name
        if not os.path.isdir(os.path.join(rootdir, dname)):
            if len(d.categories) == 1: # safety, this should always trigger
                logger.info("Deleting gone directory %s" % (dname))
                d.destroySelf()

unreadable_dirs = set()

def parent_dir(path):
    sdir = path.split('/')[:-1]
    return '/'.join(sdir)


def ctime_from_rsync(date, hms):
    year, month, day = date.split('/')
    hour, minute, second = hms.split(':')
    t = datetime(int(year), int(month), int(day), int(hour), int(minute), int(second), 0, None)
    return int(time.mktime(t.timetuple()))


def make_one_directory(line, category, path, category_directories):
    global unreadble_dirs
    readable=True
    d = line.split()[4]
    if re.compile('^\.$').match(d):
        dname = path
    else:
        dname = "%s/%s" % (path, d)
    perms = line.split()[0]
    if not re.compile('^d......r.x').match(perms) or parent_dir(dname) in unreadable_dirs:
        readable=False
        unreadable_dirs.add(dname)

    try:
        perm, size, date, hms, filepath = line.split()
    except ValueError:
        raise
    ctime = ctime_from_rsync(date, hms)
    category_directories[dname] = {'files':{}, 'isRepository':False, 'readable':readable, 'ctime':ctime, 'changed':True}
    if d.endswith('repodata'):
        parent_dname = dname[:-len('/repodata')]
        try:
            category_directories[parent_dname]['isRepository'] = True
        except KeyError:
            category_directories[parent_dname] = {'files':{}, 'isRepository':True, 'readable':readable, 'ctime':ctime, 'changed':True}
            
    return dname, category_directories

def add_file_to_directory(line, path, category_directories):
    try:
        perm, size, date, hms, filepath = line.split()
    except ValueError:
        return
    try:
        dt = ctime_from_rsync(date, hms)
    except ValueError:
        return

    l = filepath.split('/')
    filename = l[-1]
    subpath = l[:-1]
    if len(subpath) > 0:
        dirpath = ("%s/" % path) + '/'.join(subpath)
    else:
        dirpath = path
    category_directories[dirpath]['files'][filename] = {'size':size,
                                                        'stat':dt}

def short_filelist(files):
    html=0
    rpms=0
    hdrs=0
    drpms=0
    for f in files.keys():
        if f.endswith('.html'): html=html+1
        if f.endswith('.rpm'):  rpms=rpms+1
        if f.endswith('.hdr'):  hdrs=hdrs+1
        if f.endswith('.drpm'):  drpms=drpms+1
    if html>10 or rpms > 10 or hdrs > 10 or drpms > 10:
        date_file_list = []
        rc = {}
        for k in files.keys():
            date_file_tuple = (files[k]['stat'], k, files[k]['size'])
            date_file_list.append(date_file_tuple)
        date_file_list.sort()
        # keep the most recent 3
        date_file_list = date_file_list[-3:]
        
        for stat, k, size in date_file_list:
            rc[k] = files[k]
        return rc
    else:
        return files

def sync_category_directories(category, category_directories, directory_info):
    product_id = category.product.id
    topdir = category.topdir.name
    excludes=['.snapshot', '.~tmp~']
    for dirpath, value in category_directories.iteritems():
        if excludes[0] in dirpath or excludes[1] in dirpath:
            continue
        set_readable = False
        set_ctime = False
        set_files = False
        try:
            d = directory_info[dirpath]
            if d['readable'] != value['readable']: set_readable = True
            if d['ctime'] != value['ctime']:       set_ctime = True
        except KeyError:
            D = Directory(name=dirpath,readable=value['readable'], ctime=value['ctime'])
            D.addCategory(category)
            d = directory_info[dirpath] = dict(id=D.id, readable=value['readable'], ctime=value['ctime'])

        if value['changed']: set_files = True

        D = Directory.get(d['id'])
        if (set_readable or set_ctime or set_files):
            if set_readable:
                D.readable = value['readable']
            if set_ctime:
                D.ctime = value['ctime']
            if set_files:
                if D.files != short_filelist(value['files']):
                    D.files = short_filelist(value['files'])
        make_file_details_from_checksums(dirpath, D)

    # this has to be a second pass to be sure the child repodata/ dir is created in the db first
    for dirpath, value in category_directories.iteritems():
        d = directory_info[dirpath]
        D = Directory.get(d['id'])
        if value['isRepository']:
            make_repository(D, dirpath, category, product_id, topdir)
        make_repomd_file_details(dirpath, D)
    Directory.ageFileDetails()


def parse_rsync_listing(category, f):
    topdir_name = category.topdir.name
    category_directories = {}
    while True:
        line = f.readline()
        if not line: break
        line.strip()
        l = line.split()
        if line.startswith('d') and len(l) == 5 and len(l[0]) == 10: # good guess it's a directory line
            if re.compile('^\.$').match(line):
                # we know the top-level category directory already exists, don't try to re-make it
                # fixme I don't think this ever hits
                pass
            else:
                dname, category_directories = make_one_directory(line, category, topdir_name, category_directories)
        else:
            add_file_to_directory(line, topdir_name, category_directories)
    directory_info = cache_directory_info()
    sync_category_directories(category, category_directories, directory_info)

def sync_directories_using_rsync(rsyncpath, category, extra_rsync_args=None):
    result, output = run_rsync(rsyncpath, extra_rsync_args)
    if result > 0:
        logger.info("rsync returned exit code %d for Category %s: %s" % (result,
                                                                                category.name,
                                                                                output))
    # still, try to use the output listing if we can
    parse_rsync_listing(category, output)

def sync_directories_from_file(filename, category):
    f = open(filename, 'r')
    parse_rsync_listing(category, f)
    f.close()

def cache_directory_info():
    cache = dict()
    sql = "SELECT id, name, ctime, readable FROM directory"
    result = Directory._connection.queryAll(sql)
    for (id, name, ctime, readable) in result:
        cache[name] = dict(id=id, ctime=ctime, readable=readable)
    return cache

def sync_directories_from_directory(directory, category, excludes=[]):
    global unreadable_dirs
    # drop trailing slashes from path
    directory = directory.rstrip('/')
    category_directories = {}
    topdir = category.topdir
    topdirName = topdir.name
    
    directory_info = cache_directory_info()

    # fixme
    # if directory looks like /path/to/something/pub/fedora/linux
    # and topdir.name is pub/fedora/linux
    # which means we keep only [:-len(topdir.name)]
    # we want to os.walk(directory)
    # and the resulting dirpaths look like /path/to/something/pub/fedora/linux
    # and the matching directory is pub/fedora/linux.
    # paths below this then have directories of $path/dirname[len(topdir)+1:]
    stdexcludes=['.*\.snapshot', '.*/\.~tmp~']
    for dirpath, dirnames, filenames in os.walk(directory, topdown=True):
        next=False
        for e in stdexcludes + excludes:
            if re.compile(e).match(dirpath):
                next=True
                break
        if next:
            logger.info("excluding %s" % (dirpath))
            # exclude all its subdirs too
            dirnames[:] = []
            continue

        # avoid disappearing files
        try:
            s = os.stat(dirpath)
            ctime = s[stat.ST_CTIME]
        except OSError:
            continue

        i = string.index(dirpath, topdirName)
        dname = dirpath[i:]
        dname = dname.rstrip('/')
        try:
            d_ctime = directory_info[dname]['ctime']
        except KeyError:
            # we'll need to create it
            d_ctime = 0

        mode = s.st_mode
        readable = not not (mode & stat.S_IRWXO & (stat.S_IROTH|stat.S_IXOTH))
        if not readable or parent_dir(dname) in unreadable_dirs:
            unreadable_dirs.add(dname)
        isRepo = 'repodata' in dirnames

        changed = (d_ctime != ctime)
        if changed:
            logger.info("%s has changed" % dname)
        category_directories[dname] = {'files':{}, 'isRepository':isRepo, 'readable':readable, 'ctime':ctime, 'changed':changed}

        # skip per-file stat()s if the directory hasn't changed
        if changed:
            for f in filenames:
                try:
                    s = os.stat(os.path.join(dirpath, f))
                except OSError:
                    continue
                category_directories[dname]['files'][f] = {'size':str(s.st_size),
                                                           'stat':s[stat.ST_CTIME]}

    sync_category_directories(category, category_directories, directory_info)

def main():
    global rootdir
    global delete_directories
    global logger
    parser = optparse.OptionParser(usage=sys.argv[0] + " [options]")
    parser.add_option("-c", "--config",
                      dest="config", default='dev.cfg',
                      help="TurboGears config file to use")
    parser.add_option("--rootdir",
                      dest="rootdir", default=rootdir,
                      help="on-disk directory trees rooted at [rootdir]")
    parser.add_option("--logfile",
                      dest="logfile", default='/var/log/mirrormanager/umdl.log',
                      help="on-disk directory trees rooted at [rootdir]")
    parser.add_option("--debug",
                      dest="debug", default=False, action="store_true",
                      help="enable debugging")

    parser.add_option("--delete-directories",
                      dest="delete_directories", default=delete_directories, action="store_true",
                      help="delete directories from the database that are no longer on disk")

    (options, args) = parser.parse_args()
    turbogears.update_config(options.config, modulename="mirrormanager.config")
    rootdir = options.rootdir
    delete_directories = options.delete_directories

    fmt = '%(asctime)s %(message)s'
    datefmt = '%m/%d/%Y %I:%M:%S %p'
    formatter = logging.Formatter(fmt=fmt, datefmt=datefmt)
    logger = logging.getLogger('umdl')
    handler = logging.handlers.WatchedFileHandler(options.logfile, "a+b")
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    if options.debug:
        logger.setLevel(logging.DEBUG)


    if manage_pidfile(pidfile):
        logger.warning("another instance is running, try again later.")
        sys.exit(1)


    logger.info("Starting umdl")
    setup_arch_version_cache()
    for i in config.get('umdl.master_directories'):
        try:
            sync_options = i['options']
        except KeyError:
            sync_options = None

        cname = i['category']
        try:
            category = Category.byName(cname)
        except SQLObjectNotFound:
            logger.error('umdl.master_directories Category %s does not exist in the database, skipping' % (cname))
            continue            

        if category.product is None:
            logger.error('umdl.master_directories Category %s has null Product, skipping' % (cname))
            continue            

        if i['type'] == 'rsync':
            sync_directories_using_rsync(i['url'], category, sync_options)

        if i['type'] == 'file':
            sync_directories_from_file(i['url'], category)

        if i['type'] == 'directory':
            excludes = i.get('excludes', [])
            sync_directories_from_directory(i['path'], category, excludes)

    if options.delete_directories:
        nuke_gone_directories(options.rootdir)
    remove_pidfile(pidfile)
    logger.info("Ending umdl")

    return 0

if __name__ == "__main__":
    sys.exit(main())
